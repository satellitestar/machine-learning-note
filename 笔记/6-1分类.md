# 分类问题
y是离散值

eg:垃圾邮件、信用卡操作是否正常、肿瘤良性恶性？

0-负类-没有某种东西，1-正类-有某种东西

二分类->多分类，离散值的个数

## 将线性回归用于分类问题

P3

h_θ(x)=θ'x

### 只有八个样本时
线性回归假设函数如左直线  
此时输出的y时连续值，如何分为0，1？ 
令h(x)>=0.5,y=1;h(x)<0.5,y=0  
直接反推y=0.5处的x,，取值其左为良性，取值其右为恶性

结果尚可

### 添加一个样本后
现行回归假设函数"右偏"  
仍令h(x)>=0.5,y=1;h(x)<0.5,y=0 
直接反推y=0.5处的x,，取值其左为良性，取值其右为恶性

结果糟糕

### 结论&问题
线性回归不直接适用于分类问题

虽然样本中的y均为0、1，但是线性回归的假设函数可能会产生远大于1或远小于0的数，这样不好

## logistic回归算法
适用于线性回归，控制输出，0<=h(x)<=1  
虽然叫回归算法，但其实是一种分类算法

# 假设陈述

## logistic回归算法
思想：在线性回归外面套一个函数，输入为线性回归假设函数的输出，输出为0~1的值

logistic函数也称sigmoid函数

g(x)=1/[1+e^(-x)]  

函数图像类似arctanx向上平移，区间为0~1, 参见pdf

## logistic回归算法的假设函数
h_θ(x)=g(θ'x)=1/[1+e^(θ'x)]

此时假设函数输出值的含义为：
在此x情况(和此θ情况)下，y=1的概率  

可用条件概率的形式表达为：
P(y=1|x,θ)  

且有P(y=0|x,θ)=1-P(y=1|x,θ)  

# 决策边界

## 已知
h_θ(x)=g(θ'x)  
g(z)=1/[1+e^(-z)]

## 如何预测离散的结果？
当h_θ(x)>=0.5，即θ'x>=时，预测y=1  
当h_θ(x)<0.5，即θ'x<时，预测y=0

## 决策边界

### 举例1
θ'x为两参数的一次式，即  
θ'x=θ_0+θ_0*x_1+θ_0*x_2

取θ'=[-3,0,0]

则θ'x>=0的取值范围为x_1+*x_2-3>=0  
决策边界即为x_1+*x_2-3=0 
在图像上是一条斜线，右上区域为y=1区域，左下区域为y=0区域

### 举例2
θ'x=-1+x_1^2+x_2^2

决策边界为单位圆，圆外y=1，圆内y=0

## 注意
决策边界是由θ'的取值决定的，不是由样本决定的

θ'是由拟合样本的产物，但是一旦产生θ'后，决策边界就是其本身的固有属性，与样本无关了

之后讲如何拟合

# logistic算法的代价函数

## 已知
训练样本，由多个(x,y)组成  
x为n元向量  
h_θ(x)=g(θ'x)=1/[1+e^(θ'x)]  

问题：如何选择θ向量？

## 代价函数
之前学过线性回归的代价函数：  
J(θ)=∑{i=1~m}[h_θ(x_i)-y_i]²/2m

令Cost(h_θ(x),y)=[h_θ(x_i)-y_i]²/2  

J(θ)=∑{i=1~m}Cost(h_θ(x),y)/m

这是代价函数的一般形式？？各个样本代价的平均值

此时的问题，如果Cost函数继续用线性回归的函数形式，产生的J(θ)-θ函数会有多个局部最优解，不是凸函数，使用梯度下降不能正确的收敛到全局最优解，图像见pdf

我们希望有一种新的Cost函数，来使J(θ)-θ函数使凸函数，仅有一个全局最优解，可以使梯度下降算法正常运行

## logistic算法的Cost函数
y=1时，Cost(h_θ(x),y)=-log(h_θ(x))
y=0时，Cost(h_θ(x),y)=-log(1-h_θ(x))

图像见pdf

这个函数满足了“预测值与实际值接近，代价越小；预测值与实际值越远，代价越大”，此时预测值虽然是线性的，但是在0~1的范围内，实际值只有0、1

举例
- y=1时，假设函数输出了1，Cost=0，预测的准，代价很小
- y=1时，假设函数输出了0，Cost=∞，预测的不准，代价很大
- y=0亦然........

# 简化代价函数与梯度下降

## Cost函数的化简
转化为一个函数，完全等价
Cost(h_θ(x),y)=-y*log(h_θ(x))-(1-y)*log(1-h_θ(x))

## logistic回归的代价函数
J(θ)=∑{i=1~m}Cost(h_θ(x),y)/m  
J(θ)=∑{i=1~m}{-y*log(h_θ(x))-(1-y)*log(1-h_θ(x))}/-m

目标，最小化J(θ)

## logistic回归的梯度下降
和线性回归的梯度下降迭代公式相同
θ_j=θ_j-α*"J(θ)对θ_j的偏导"

求偏导的过程自己算过了，注意负号和新的假设函数

将偏导带入后的迭代公式和线性回归中的迭代公式相同

### 注意
- 假设函数和代价函数都发生了变化
- 可以用向量化实现一次更新所有的θ值
- 特征缩放方法适用次梯度下降算法，可以加速收敛

# 高级优化

## 更好的优化代价函数的算法
- conjugate gradient
- BFGS
- L-BFGS

优点
- 不需要手动选择学习率α
- 比梯度下降算法执行的更快

缺点
- 复杂(但是不需要你去完全搞懂)

## 使用Octave第一次实现梯度下降
需要自己创建一个costFunction函数，接受θ向量，在内部把代价函数手动写出来、手动计算的偏导值并写出来，并把二者返回出去

再使用Octave中的fminunc(这个就是一种高级算法)，输入costFunction、初始的θ向量、配置，fminunc回自己找到costFunction返回的两个参数，自动进行迭代工作，输出最后的θ向量、此时J(θ)的值，是否收敛的标志

详情参照<https://blog.csdn.net/gzp444280620/article/details/49272977>

# 多元分类问题
举例：邮件分为家人、朋友、公司、垃圾；天气分为晴天、下雨、下雪、多云；......

思路：将n元分类问题转化为n个二元分类问题。每次将一个结果分为正类，另外n-1个结果分为负类，然后进行二元分类，重复n次。

得出n个假设函数，见pdf倒数第二页最后
n个假设函数，各有各的θ向量

h_θ^i(x)=P(y=i|x,θ) i=1~n  
h_θ^i(x)代表在取这个x时，依据之一组θ得出的假设函数，y分类为i的概率是多少。

当面对一个输入x时，计算n个h_θ^i(x)值，选出其中最大的那个(代表分到该类的概率最大)，其对应的i值即为分类结果






















