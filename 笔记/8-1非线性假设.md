# 非线性假设
为什么要使用非线性假设？

面对一个分类问题，当n=100时，如果用逻辑回归模型，假设函数的一次项有100个，二次相乘项有O(n²)个，二次平方项有100个。  
如果只考虑二次平方项，可以做，但是不能拟合复杂的图形  
更不要说后面还有三次项，数量为O(n³)个，越170000个.......

这么多特征值真的不好处理！

举例：计算机视觉对车辆进行识别  
一张黑白图50*50像素=2500个像素，还只是一次项，如果是彩色图有7500个像素。

我们要使用线性假设来学习的话，至少得是二次项，特征值之间相乘有三百万个特征值

计算成本太高了！！

于是用神经网络来学习复杂的非线性假设

# 神经元与大脑
介绍并展望了一下神经网络，他由对大脑的模仿而来，可以用于机器学习。

# 模型展示I

## 脑中的神经元
一个计算单位，由多个输入，一个输出

## 电脑中的逻辑单元

注意：这里不知道是不是叫逻辑单元就代表里面的处理函数就是逻辑回归模型中的假设函数，logistic/sigmoid函数，但是课里是这样说的

一个x向量的输入，1个假设函数值得输出

此处可以有固定为1的x_0，任何层可以都有，可写可不写，这个是和θ_0相乘的那个x

假设函数值就是逻辑回归模型中输出的假设函数

这是一个带有logistic激活函数的神经元

激活函数即指代非线性函数g(z)的另一个术语

三层的神经网络
1. 输入层
2. 隐藏层
3. 输出层

## 表示方法
- a下标i上标j  
表示在第j层的第i个单元的“激活项”(激活项就是因变量、假设函数的输出？)
- s下标j
表示在j层有s_j个元素
- 大写θ上标j  
表示从j层往j+1层算的时候，权重θ组成的矩阵  
j层有s_j个元素，j+1层有s_(j+1)个元素，矩阵为行数为s_(j+1)行，列数为s_j+1列的矩阵  
展开式看pdf最后一页，输出的个数作为行数，输入的个数+1(加上θ_0)作为列数
- 在公式里的θ上标j下标pq  
j时这个权重所在的层号，p代表这是再算第p个输出项，q代表他和第q个x相乘  

有点复杂！！！




